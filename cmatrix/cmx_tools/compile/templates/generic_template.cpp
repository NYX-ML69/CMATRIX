/*
 * Generated inference code for Generic target
 * Model: $model_name
 * Target: $target
 * Generated by cmx_tools
 */

$includes
#include <iostream>
#include <cstdint>
#include <cstring>
#include <chrono>
#include <vector>
#include <memory>
#include <cmatrix.hpp>

$defines

/*
 * Static tensor declarations
 * Generic implementation with standard C++ containers
 */
$tensor_declarations

// Model weights and biases
static const std::vector<float> model_weights = {
    // Weights will be populated during code generation
    0.0f
};

static const std::vector<float> model_biases = {
    // Biases will be populated during code generation
    0.0f
};

// Working memory buffers
static std::vector<float> input_buffer;
static std::vector<float> output_buffer;
static std::vector<float> temp_buffer;

/*
 * Generic utility functions
 */
class Timer {
private:
    std::chrono::high_resolution_clock::time_point start_time;
    
public:
    void start() {
        start_time = std::chrono::high_resolution_clock::now();
    }
    
    double elapsed_ms() {
        auto end_time = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(
            end_time - start_time);
        return duration.count() / 1000.0;
    }
    
    double elapsed_us() {
        auto end_time = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(
            end_time - start_time);
        return duration.count();
    }
};

/*
 * Memory management utilities
 */
template<typename T>
class AlignedAllocator {
public:
    using value_type = T;
    
    AlignedAllocator() = default;
    
    template<typename U>
    AlignedAllocator(const AlignedAllocator<U>&) {}
    
    T* allocate(std::size_t n) {
        void* ptr = nullptr;
        if (posix_memalign(&ptr, ALIGNMENT, n * sizeof(T)) != 0) {
            throw std::bad_alloc();
        }
        return static_cast<T*>(ptr);
    }
    
    void deallocate(T* ptr, std::size_t) {
        free(ptr);
    }
    
    template<typename U>
    bool operator==(const AlignedAllocator<U>&) const { return true; }
    
    template<typename U>
    bool operator!=(const AlignedAllocator<U>&) const { return false; }
};

using AlignedFloatVector = std::vector<float, AlignedAllocator<float>>;

/*
 * Layer implementation functions
 */
$layer_functions

/*
 * Generic optimized convolution
 */
void generic_conv2d_optimized(const float* input, float* output, 
                             const float* weights, const float* bias,
                             int in_channels, int out_channels, int kernel_size) {
    // Basic optimized implementation using loop unrolling
    for (int oc = 0; oc < out_channels; oc++) {
        float acc = bias[oc];
        
        for (int ic = 0; ic < in_channels; ic++) {
            const float* weight_ptr = &weights[oc * in_channels * kernel_size * kernel_size + 
                                             ic * kernel_size * kernel_size];
            const float* input_ptr = &input[ic * kernel_size];
            
            // Unroll inner loop for better performance
            for (int k = 0; k < kernel_size * kernel_size; k += 4) {
                acc += input_ptr[k] * weight_ptr[k];
                if (k + 1 < kernel_size * kernel_size)
                    acc += input_ptr[k + 1] * weight_ptr[k + 1];
                if (k + 2 < kernel_size * kernel_size)
                    acc += input_ptr[k + 2] * weight_ptr[k + 2];
                if (k + 3 < kernel_size * kernel_size)
                    acc += input_ptr[k + 3] * weight_ptr[k + 3];
            }
        }
        
        output[oc] = acc;
    }
}

/*
 * Model class for better encapsulation
 */
class ${model_name}Model {
private:
    bool initialized;
    Timer timer;
    
    // Statistics
    uint64_t inference_count;
    double total_inference_time_ms;
    
public:
    ${model_name}Model() : initialized(false), inference_count(0), total_inference_time_ms(0.0) {}
    
    ~${model_name}Model() {
        cleanup();
    }
    
    int init() {
        if (initialized) {
            return 0;  // Already initialized
        }
        
        std::cout << "Initializing " << "${model_name}" << " model..." << std::endl;
        
        // Initialize cmx_core library
        if (cmx_init() != CMX_SUCCESS) {
            std::cerr << "Failed to initialize cmx_core" << std::endl;
            return -1;
        }
        
        // Allocate working buffers
        try {
            input_buffer.resize(get_input_size());
            output_buffer.resize(get_output_size());
            temp_buffer.resize(2048);  // Temporary buffer for intermediate results
            
            // Clear buffers
            std::fill(input_buffer.begin(), input_buffer.end(), 0.0f);
            std::fill(output_buffer.begin(), output_buffer.end(), 0.0f);
            std::fill(temp_buffer.begin(), temp_buffer.end(), 0.0f);
            
        } catch (const std::exception& e) {
            std::cerr << "Failed to allocate memory: " << e.what() << std::endl;
            return -1;
        }
        
        initialized = true;
        std::cout << "Model initialized successfully" << std::endl;
        return 0;
    }
    
    void cleanup() {
        if (!initialized) {
            return;
        }
        
        std::cout << "Cleaning up model..." << std::endl;
        
        // Clear buffers
        input_buffer.clear();
        output_buffer.clear();
        temp_buffer.clear();
        
        cmx_shutdown();
        initialized = false;
    }
    
    int inference(const float* input, float* output) {
        if (!initialized) {
            std::cerr << "Model not initialized" << std::endl;
            return -1;
        }
        
        if (input == nullptr || output == nullptr) {
            std::cerr << "Invalid input or output pointer" << std::endl;
            return -1;
        }
        
        timer.start();
        
        // Copy input to working buffer
        std::memcpy(input_buffer.data(), input, get_input_size() * sizeof(float));
        
        // Execute inference pipeline
$inference_loop
        
        // Copy result to output buffer
        std::memcpy(output, output_buffer.data(), get_output_size() * sizeof(float));
        
        // Update statistics
        double elapsed = timer.elapsed_ms();
        inference_count++;
        total_inference_time_ms += elapsed;
        
        return 0;
    }
    
    // Metadata functions
    const char* get_version() const {
        return "1.0.0-generic";
    }
    
    int get_input_size() const {
        return 224 * 224 * 3;  // Will be replaced with actual size
    }
    
    int get_output_size() const {
        return 1000;  // Will be replaced with actual size
    }
    
    // Performance statistics
    void print_stats() const {
        std::cout << "\\nPerformance Statistics:" << std::endl;
        std::cout << "  Total inferences: " << inference_count << std::endl;
        std::cout << "  Total time: " << total_inference_time_ms << " ms" << std::endl;
        
        if (inference_count > 0) {
            double avg_time = total_inference_time_ms / inference_count;
            std::cout << "  Average time: " << avg_time << " ms" << std::endl;
            std::cout << "  Throughput: " << (1000.0 / avg_time) << " inferences/sec" << std::endl;
        }
    }
    
    void reset_stats() {
        inference_count = 0;
        total_inference_time_ms = 0.0;
    }
    
    bool is_initialized() const {
        return initialized;
    }
};

// Global model instance
static ${model_name}Model g_model;

/*
 * C-style API for compatibility
 */
extern "C" {

int ${model_name}_init(void) {
    return g_model.init();
}

void ${model_name}_cleanup(void) {
    g_model.cleanup();
}

int ${model_name}_inference(const float* input, float* output) {
    return g_model.inference(input, output);
}

const char* ${model_name}_get_version(void) {
    return g_model.get_version();
}

int ${model_name}_get_input_size(void) {
    return g_model.get_input_size();
}

int ${model_name}_get_output_size(void) {
    return g_model.get_output_size();
}

}  // extern "C"

/*
 * Benchmark utilities
 */
class Benchmark {
private:
    ${model_name}Model& model;
    std::vector<float> input_data;
    std::vector<float> output_data;
    
public:
    Benchmark(${model_name}Model& m) : model(m) {
        input_data.resize(model.get_input_size());
        output_data.resize(model.get_output_size());
        
        // Initialize with random-like data
        for (size_t i = 0; i < input_data.size(); i++) {
            input_data[i] = 0.1f * (i % 100) - 5.0f;  // Range: -5.0 to 4.9
        }
    }
    
    void run_warmup(int iterations = 5) {
        std::cout << "Running warmup (" << iterations << " iterations)..." << std::endl;
        
        for (int i = 0; i < iterations; i++) {
            model.inference(input_data.data(), output_data.data());
        }
        
        model.reset_stats();  // Reset stats after warmup
    }
    
    void run_benchmark(int iterations = 100) {
        std::cout << "Running benchmark (" << iterations << " iterations)..." << std::endl;
        
        Timer benchmark_timer;
        benchmark_timer.start();
        
        for (int i = 0; i < iterations; i++) {
            int result = model.inference(input_data.data(), output_data.data());
            if (result != 0) {
                std::cerr << "Inference failed at iteration " << i << std::endl;
                return;
            }
            
            // Print progress
            if ((i + 1) % 10 == 0) {
                std::cout << "  Completed " << (i + 1) << "/" << iterations << " iterations" << std::endl;
            }
        }
        
        double total_time = benchmark_timer.elapsed_ms();
        
        std::cout << "\\nBenchmark Results:" << std::endl;
        std::cout << "  Total iterations: " << iterations << std::endl;
        std::cout << "  Total time: " << total_time << " ms" << std::endl;
        std::cout << "  Average time per inference: " << (total_time / iterations) << " ms" << std::endl;
        std::cout << "  Throughput: " << (iterations * 1000.0 / total_time) << " inferences/sec" << std::endl;
        
        // Print sample outputs
        std::cout << "\\nSample outputs (first 10 values):" << std::endl;
        for (int i = 0; i < 10 && i < model.get_output_size(); i++) {
            std::cout << "  [" << i << "] = " << output_data[i] << std::endl;
        }
    }
};

/*
 * Command line argument parsing
 */
struct CommandLineArgs {
    bool run_benchmark = false;
    int benchmark_iterations = 100;
    int warmup_iterations = 5;
    bool verbose = false;
    bool show_help = false;
};

CommandLineArgs parse_args(int argc, char* argv[]) {
    CommandLineArgs args;
    
    for (int i = 1; i < argc; i++) {
        std::string arg = argv[i];
        
        if (arg == "--benchmark" || arg == "-b") {
            args.run_benchmark = true;
        } else if (arg == "--iterations" || arg == "-i") {
            if (i + 1 < argc) {
                args.benchmark_iterations = std::atoi(argv[++i]);
            }
        } else if (arg == "--warmup" || arg == "-w") {
            if (i + 1 < argc) {
                args.warmup_iterations = std::atoi(argv[++i]);
            }
        } else if (arg == "--verbose" || arg == "-v") {
            args.verbose = true;
        } else if (arg == "--help" || arg == "-h") {
            args.show_help = true;
        }
    }
    
    return args;
}

void print_help(const char* program_name) {
    std::cout << "Usage: " << program_name << " [options]\\n" << std::endl;
    std::cout << "Options:" << std::endl;
    std::cout << "  -h, --help               Show this help message" << std::endl;
    std::cout << "  -b, --benchmark          Run benchmark mode" << std::endl;
    std::cout << "  -i, --iterations <n>     Number of benchmark iterations (default: 100)" << std::endl;
    std::cout << "  -w, --warmup <n>         Number of warmup iterations (default: 5)" << std::endl;
    std::cout << "  -v, --verbose            Enable verbose output" << std::endl;
    std::cout << std::endl;
    std::cout << "Examples:" << std::endl;
    std::cout << "  " << program_name << "                    # Run single inference" << std::endl;
    std::cout << "  " << program_name << " --benchmark       # Run benchmark with default settings" << std::endl;
    std::cout << "  " << program_name << " -b -i 1000        # Run benchmark with 1000 iterations" << std::endl;
}

/*
 * Main function
 */
int main(int argc, char* argv[]) {
    // Parse command line arguments
    CommandLineArgs args = parse_args(argc, argv);
    
    if (args.show_help) {
        print_help(argv[0]);
        return 0;
    }
    
    std::cout << "Generic Neural Network Inference" << std::endl;
    std::cout << "Model: ${model_name}" << std::endl;
    std::cout << "Version: " << g_model.get_version() << std::endl;
    std::cout << "Input size: " << g_model.get_input_size() << " elements" << std::endl;
    std::cout << "Output size: " << g_model.get_output_size() << " elements" << std::endl;
    std::cout << std::endl;
    
    // Initialize model
    if (g_model.init() != 0) {
        std::cerr << "Failed to initialize model" << std::endl;
        return -1;
    }
    
    try {
        if (args.run_benchmark) {
            // Run benchmark
            Benchmark benchmark(g_model);
            benchmark.run_warmup(args.warmup_iterations);
            benchmark.run_benchmark(args.benchmark_iterations);
            
        } else {
            // Run single inference
            std::vector<float> input(g_model.get_input_size());
            std::vector<float> output(g_model.get_output_size());
            
            // Initialize with dummy data
            for (size_t i = 0; i < input.size(); i++) {
                input[i] = 0.1f * (i % 10);
            }
            
            std::cout << "Running single inference..." << std::endl;
            
            Timer timer;
            timer.start();
            
            int result = g_model.inference(input.data(), output.data());
            
            double elapsed = timer.elapsed_ms();
            
            if (result == 0) {
                std::cout << "Inference completed successfully" << std::endl;
                std::cout << "Execution time: " << elapsed << " ms" << std::endl;
                
                // Print first few outputs
                std::cout << "\\nOutput (first 10 values):" << std::endl;
                for (int i = 0; i < 10 && i < g_model.get_output_size(); i++) {
                    std::cout << "  [" << i << "] = " << output[i] << std::endl;
                }
                
            } else {
                std::cerr << "Inference failed with error code: " << result << std::endl;
                return -1;
            }
        }
        
        // Print model statistics
        if (args.verbose) {
            g_model.print_stats();
        }
        
    } catch (const std::exception& e) {
        std::cerr << "Exception occurred: " << e.what() << std::endl;
        g_model.cleanup();
        return -1;
    }
    
    // Cleanup
    g_model.cleanup();
    
    std::cout << "\\nApplication finished successfully" << std::endl;
    return 0;
}
